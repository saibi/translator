# 출처 및 번역 정보
- **원문 URL:** https://joshcollinsworth.com/blog/sloptimism
- **번역:** Cursor AI (기술 문서 번역)

---

# AI 낙관론은 계급 특권이다

**게시일:** 2025년 12월 3일  
**수정일:** 2025년 12월 11일

얼마 전, AI가 조금 더 이른 시대에[^1], GitHub 프로필을 읽어서 내용을 바탕으로 맞춤형 디스(놀림)를 만들어 주는 프로젝트가 돌아다녔다.

무해하고 가벼운 재미를 위한 것이라고 나는 추측한다. 나도 재미에 끼고 싶어서 내 프로필을 넣고 써봤다.

나는 내 디스를 보고 웃지 않았다.

영리하지도, 재밌지도, 특히 예상 밖이지도 않았다. Fiverr에서 테크에 능한 낯선 사람이 더 잘했을 것 같다.

그보다 더한 건, 나는 그게 얼마나 _심하게_ 굴렀는지에 놀랐다는 거다. 모델이 만들어 낸 것 중 조금만이 농담처럼 느껴졌고, 대부분은 아주 개인적인 모욕이 쏟아지는 글로 읽혔다.

그다음에는, 그 인공적인 잔인함이 실제로 _나에게_ 영향을 줬다는 사실에 놀랐다.

이게 전부 영혼 없는(그리고 지금 보니 유머 감각도 없는) 기계가 서툰 코미디를 시도한 것이고, 다른 누구도 보지 않았다는 걸 알면서도, 그 문장들을 읽는 게 아팠다. 이상하게도, AI가 실제로 나를 _다치게_ 한 셈이다.

그때 처음으로, AI가 내 _아이들_ 에게 어떤 영향을 줄지 생각하게 됐다.

수십 년의 인터넷 사용으로 단단해진, 꽤 두꺼운 피부를 가진 어른인 내가 여전히 고도로 맞춤화된 온라인 괴롭힘에 상처받을 수 있다면, 심술궂은 아이가 이 기술을 손에 넣고 악용하기로 마음먹었을 때 내 아들의 처지는 어떨까?

아이들이 _진짜_ 괴롭힘을 겪을 때쯤이면, 멸시적인 농담은 AI가 가능하게 할 적대 행위 중 _가장 덜_ 해로운 형태에 불과할 거라고 나는 확신한다. 딥페이크로 한 명의 나쁜 아이가 어떤 피해를 줄 수 있는지 상상해 보라. 못된 소문을 퍼뜨리던 시대는 잊어라; 이제는 그게 일어나는 영상을 공유할 수 있다.

AI가 잔인한 청소년에게—특히 기술이 몇 해 더 발전한 _후에는_—수치심, 위협, 괴롭힘, 트라우마를 가할 수 있게 할 수 있다고 상상해 보라. (비윤리적인 어른이 할 수 있는 일은 말할 것도 없다.)

그 피해가 얼마나 막기 어려울지 상상해 보라.

내 반응은 웃음이 아니었다. 우리가 완벽한 괴롭힘 도구를 만들기 위해 달리고 있다는 걸 깨닫은 _공포_ 였다.

> 내 반응은 웃음이 아니었다. 내 반응은 우리가 완벽한 괴롭힘 도구를 만들기 위해 달리고 있다는 깨달음에 대한 공포였다.

나는 AI에 관해 원래부터 낙관주의자는 아니었지만, 내가 예견한 미래가 그렇게 어두울 수 있다는 걸 그때 처음으로 똑바로 깨달았다.

---

완전히 정확한 표현은 아니지만, 이 글에서는 “AI 낙관주의자”라는 말을 자주 쓸 것이다. 최소한 한 집단을 가리키는 쓸 만한 꼬리표이기 때문이다.

그 집단을 조금 더 구체적으로 말하면, AI에 흥분하는 사람들이다. 미래 발전을 포함할 수 있지만, 특히 _지금 그리고 가까운 미래의_ AI, 그리고 지금 당장 어떻게 쓸 수 있는지에 특히 흥분한다. 열성파나 심지어 신봉자라고 부를 수도 있다. 어쨌든 대체로 AI에 열광하고, 비용이나 부작용을 크게 걱정하지 않는다.

이런 사람을 한두 명쯤은 알고 있을 거다. 당신이 그중 하나일 수도 있다. (그렇다면: 이 글로 마음을 바꿀 거라고 생각하진 않지만, 생각할 거리를 조금은 드리길 바란다.)

이 집단에 속하려면—현재 존재하는 AI를 낙관과 열광으로 바라보려면—최소한 어느 정도의 특권이 필요하다고 나는 본다. 그래서 이 글 제목이 다소 무딘 표현을 쓰는 것이다.

이 생각을 오래도록 말로 잘 담지 못했는데, 이 글 제목 문장으로 굳어지자 주변 많은 것이 갑자기 정리되는 느낌이 들었다.

그래서 이 글을 썼다. 그 시각을 나누려고. 내 시각이고, 내 경험(그리고 네, 나 자신의 상당한 특권—계급적이든 다른 것이든—을 통한 렌즈)에서 나온다. 받아들이든 말든 자유다.

---

2025년 말이다. AI에 대한 극단적으로 대립하는 시각이 얼마나 극단적인지 굳이 말할 필요 없다. _누구나_ AI에 대한 의견이 있고, 압도적 다수는 스펙트럼의 이쪽 끝이나 저쪽 끝에 있다. 양쪽 사이에는 큰 극명이 있고, 각각 정반대 방식으로 맹렬히 열정적이다.

나로서는 여러 이유로 확실히 비관 쪽에 선다. 그중 일부는 여기서 다루고, 일부는 다른 데서 이미 잘 다뤄져서 대부분 건너뛴다.

지금은 이렇게만 하자. AI가 _현재_ 미치는 영향을 둘러보면, 나는 열광할—더 나아가 이 근본적으로 결함 있는 기술이 언젠가 곧 경제·사회·과학 혁명을 스스로 일으킬 수 있다는 거의 의문되지 않는 준종교적 믿음을 가질—이유를 거의 찾지 못한다.

생각해 보니 “종교적”이란 말이, 밖에서 보는 AI 낙관론을 설명하기에 적절할 수 있겠다. 열렬한 신봉자, 저명한 인물들의 믿음으로 받아들여야 할 예언, 그리고 물론 어떤 종교나 그렇듯 질문이 뭐든 간에 항상 “정답”으로 바칠 수 있는 중심적인 숭배 대상이 있다.

공정하게 말하면, AI 낙관주의자 전부가 그런 건 아니다. 주로 극단적인 케이스를 말하는 것이다.

그래도 더 온건한 낙관주의자들—그냥 기술을 좋아하는 보통 사람들—가운데서도 그 열광은 항상… 과한 편인 것 같았다.

같은 기술, 같은 결과를 보면서 전혀 다른 결론을 내리는 동료들이 그렇게 많다는 게 항상 이해가 안 갔다. 내가 느끼는 불안한 결함과 단점에는 무관심하고, 나보다 AI에 훨씬 더 감명받아서 나와는 반대편에 서고 싶어 하는 것 같았다.

그들은 나와 특별히 다르지 않아 보였다. 오히려 많은 이가 친구, 인맥, 존경하는 사람들이었다.

우리는 같은 기술, 같은 결과를 보면서 전혀 다른 결론을 내리고 있었다. 내가 놓치는 게 뭘까?

나중에 답이 떠올랐다.

그들은 AI의 _수혜자_ 로 자신을 보고 있었지, _대가를 치를_ 쪽이 아니었다.

---

AI가 어떤 작업에 가끔 도움이 될 수 있다는 건 인정하고, 그 범위에서의 열광은 이해한다. 자주 쓰진 않지만 솔직히 조금은 쓴다. (이 블로그 글의 한 글자 한 글자는 여전히 내가 직접 쓴다. em dash까지 전부.)

일러스트 참고용 이미지를 만들 때 출발점으로 쓰거나, 가끔 “고무 오리”처럼 아이디어를 낼 때 도움이 된다. 색각 이상 보정용으로 가끔 쓰기도 한다. 하지만 주로 코드에서 도움을 받는다.

사소한 디테일을 다 밝히면, VS Code의 코드 완성 제안을 주로 쓰고, 맞을 때도 있고 아닐 때도 있다. 채팅 모드는 거의 안 쓰고, 쓸 때도 포맷 변환이나 패턴 매칭 같은 반복 작업 위주다. 그게 전부다. AI에게 그 이상의 역할을 맡길 때마다 꽤 spectacular하게 실망했다.

AI가 전반적으로 순생산성을 올린다고 나는 깊이 회의적이고[^2], 특히 프론트엔드 코드의 고품질을 낼 수 있다고는 생각하지 않는다. 좋은 프론트엔드는 너무 주관적이고, 시각적이고, 고려할 게 많고, 학습 데이터에 너무 적게 나타난다고 가설을 세운다. (다른 분야 개발자들이 더 나은 결과를 보고하는 이유일 수 있다.)

이제 열성파들이 “올바른 방식”으로 AI를 쓰지 않는다고 코웃음 치며 나가려는 소리가 들린다. Cursor 에이전트나 MCP로 vibe-coding하거나 그 주마다 유행하는 걸 말이다. 맞다. 그렇게 깊이 써본 적은 없다.

한편으로는 유출된 비밀, 삭제된 DB, 초기화된 하드 드라이브 같은 소름 끼치는 이야기를 너무 많이 들어서다. 비결정론적 블랙박스에게 내 머신이나 프로덕션 전체를 맡기는 생각이 싫다.

그리고 _두뇌를 쓰는 것_ 이 좋다. 내가 하는 일에 대한 열정은 대부분 아이디어를 내고, 만들고, 창의적으로 문제를 푸는 과정에서 온다. 기계가 그걸 다 해주고 결과만 주는 건, 이미 다 풀린 스도쿠 책이나 다른 사람이 3분의 2까지 해둔 세이브 파일을 불러오는 것만큼 만족스럽지 않다. _결과_ 만 원해서 하는 게 아니라 _경험_ 도 원한다.

_나_ 가 성장하고 싶다. 기술을 실제로 쓰지 않으면 그게 어떻게 일어날지 상상하기 어렵다.

이 모든 건 어차피 부수적인 얘기다. AI에 대한 내 문제는 그 효과성 수준과 거의 관계없다.

---

새로운 코딩 동료가 심하게 과신하는 편이어도, 예전엔 시간이 걸리거나 어려웠을 작업을 빠르고 쉽게 해주면 당연히 신날 수 있다.

하지만 그걸로 AI 낙관주의자가 되려면, _그다음은_ 생각을 멈춰야 한다.

내가 안 보고 있을 때 그 작은 코딩 동료가 뭘 하는지, _다른_ 사람들의 삶에 어떤 _다른_ 영향을 주는지를 무시해야 한다.

레이오프를 예로 들자.

AI 낙관주의자가 되려면, 자동화나 인원 감축 위험에 있는 직군에 당신이 속하지 않았다고, 수많은 이직당할 노동자 가운데 당신이 없다고 믿어야 할 것 같다. (최소한, 일의 흥미롭고 보람 있는 부분을 AI가 가져가고 당신 역할이 그걸 관리하는 쪽으로 줄어드는 위험은 없다고 믿어야 한다.) 당신 자신의 생계에 대한 위협에 어떻게 열광할 수 있겠는가?[^3]

조직도에서 충분히 위에 있어야 하고, 피라미드에서 충분히 높은 곳에, 커리어 사다리에서 충분히 올라가 있어야 한다.

AI 낙관주의자가 되려면, 다음 직장이 어디서 나올지, 아예 구할 수 있을지 걱정하지 않는 사람이어야 할 것이다. 현재 취업 시장의 어려운 상황이 당신을 위협하지 않는다고 느껴야 한다. 안전하다고 느껴야 한다.

이미 이름을 떨쳤을 수도 있다. 컨퍼런스나 팟캐스트에서 알려졌을 수도 있다. 이력서가 문을 열어줄 만큼 시니어일 수도 있다.

아니면 리더십으로 승진했을 수도 있다. 중요한 회의로 하루를 보낼 수도 있다.

앞에 적절한 차트와 그래프가 있으면 낙관하기가 훨씬 쉬울 수도 있다.

당신은 거의 확실히 주니어나 인턴이 아니고, 이 분야에 뛰어들려는 사람도 아닐 것이다. 내 업계와 여러 다른 업계의 주니어를 삼키는 rising tide 근처에 있으면 안 된다. 악명대로, 이제 _주니어를 뽑는 곳이_ 거의 없다.[^4]

그들을 벽에 붙이는 것을 열광하고 있다면, 당신이 그 “첫 번째 그룹”에 속하지 않았다고 보는 게 안전하다.

> 그들을 벽에 붙이는 것을 열광하고 있다면, 당신이 그 첫 번째 그룹에 속하지 않았다고 보는 게 안전하다.

계약직이나 컨설팅 회사 직원일 가능성도 낮다. 그리고 거의 _확실히_ 예술가, 일러스트레이터, 작가가 아니다. 클라이언트 돈이 위로 흘러가고, 당신의 수입을 갉아먹는 이게 당신과 당신과 같은 일을 하는 수백만 명을 뻔뻔하게 표절한 덕분에만 가능하다는 쓰라린 걸 보지 않았을 것이다.

AI 낙관론은 아마 _당신_ 작업을 훔치는 사람이 없고, _당신_ 직군 전체가 밀려나지 않는 위치에 있다는 뜻이다.

AI에 낙관하는 건 이렇다. 당신에게 주는 이득에 집중하려면, 다른 이들에게 치르는 대가는 무시해야 한다.

> AI에 낙관하는 건 이렇다. 당신에게 주는 이득에 집중하려면, 다른 이들에게 치르는 대가는 무시해야 한다.

AI 낙관론은 LLM 사용으로 정신병, 폭력, 심지어 자살로 내몰릴 사람들 가운데 당신(과 당신이 아끼는 사람들)이 없다고 믿는 것을 요구한다. 최소한 당신 자신의 정신 건강에 대해 안전하다고 느껴야 하고, 아마 더 넓고 튼튼한 지원망이 당신을 받쳐 준다고 느껴야 한다.

(너무 직접적으로 말하면, 그런 것들은 다른 말로 “특권”이라고 부른다.)

AI 낙관론은 확장하는 데이터센터, 막대한 전력 수요, 물 소비, AI 붐의 다른 환경 위험에[^5] 영향을 받을 사람이 누구든 _당신이 아니라고_ 믿는 것을 요구한다. 어떤 재앙이 닥치든 당신 동네는 안전하다. 아마 멀리 떨어져 있을 것이다.

---

AI의 해악은 단독 이슈가 아니다. AI가 _다른_ 기술, 시스템, 사회 일부에 들어가면서, 기존 문제를 악화하고 이미 다른 곳에서 일어나고 있는 피해를 가속한다.

사기꾼들은 AI에 열광할 거라고 나는 믿는다. 사기에 그렇게 유용한 도구는 없었을 것이다. 범죄자와 사기꾼은 항상 있었지만, 이렇게 강력한 수단을 가진 적은 없었다. 의심하지 않는 할머니를 속이려면 그 사람의 영상을 허공에서 만들어 내거나 전화에서 목소리를 완벽히 흉내 내면 되니까.[^6]

하지만 그건 개인을 노리는 상대적으로 작은 규모의 해악이다. 더 넓은 해악은 AI가 정부와 그 하부 구조 같은 _시스템_ 과 만나면서 생긴다.

악의적인 국가 행위자(미국 내외)가 AI를 무자비하게 효율적인 선전 도구로 써서, 이전보다 더 설득력 있고 이전에는 불가능했던 속도로 허위정보를 퍼뜨리고 있다. 그중 상당수는 이민자, 난민, 퀴어, 정치적 반대자 같은 취약 집단을 비인간화하고 희생시키는 데 쓰인다. 주로(하지만 독점적으로는 아님) 권위주의 권력을 강화하려는 목적이다.

그런 공포를 가능하게 하는 기술에 낙관하기가 어렵다고 생각하지만, 그게 아마 당신에게는 영향을 안 줄 거라고 아는 게 도움이 되겠지.

그 기술이 나나 내 주변 사람을 폭력의 표적로 만드는 데 쓰인다면, 그 기술이 이메일을 더 빨리 쓰게 해준다고 기분이 좋을 수 있을지 모르겠다.

의도가 좋을 때도 AI는 기존 해악을 증폭하는 경우가 많다.

가능한 모든 곳에 AI를 밀어 넣는 과정에서, 이제 사법 시스템에도 주입했다. 얼굴 인식·감시 기술부터 데이터·행정 업무, 법 시스템까지 들어가 있다.

이론상으로는 효율 향상이다. _이론상_ 기계는 인간보다 덜 편향되어야 한다.

현실에서는, 이 모델들은 이 맥락에서 전혀 받아들일 수 없는 비율로 실수를 하고, 학습 데이터에 이미 있는 인종주의를 모방·증폭한다. (기술은 항상 만든 이의 거울이지, 중립이 아니다.) 게다가 AI는 비결정론적이고 블랙박스에 가까워서, 결과를 검사·이의제기·항소할 방법이 거의 없다.

말할 필요 없이, AI의 이런 투입은 이미 실제 사람들의 삶에 심각한 피해를 줬고, 그 피해는 줄어들 기미가 없다.

용서해 달라. 불평등을 빠르게 가속하는 이 기술이 코드 쓰는 시간을 조금 줄여준다고 흥분할 수는 없겠다.

그런 흥분을 느끼려면, 이 중 어느 것도 _나_ 에게나 내게 중요한 사람에게는 일어나지 않을 거라고 생각해야 할 것 같다.

아니면 최소한, 모두 불행하지만 궁극적으로 더 큰 선을 위한 것이라고. 정당한 트레이드오프, 나중에 고칠 버그라고.

AI 낙관론은 동료 인간 중 최소한 일부의 삶을 가치 있는 희생, 백로그의 버그 리포트로 보는 것을 요구한다.

> AI 낙관론은 동료 인간 중 최소한 일부의 삶을 가치 있는 희생, 백로그의 버그 리포트로 보는 것을 요구한다.

하지만 더 큰 시스템이 뒤에 없고, 악의적이든 아니든 더 넓은 목표나 의제가 전혀 없어도, AI는 기존 해악을 증폭할 수 있다.

내가 떠올리는 한 예: 페이스북이 최근 AI로 생성된 여성을 격하게 교살하는 영상으로 도배됐다. 이 끔찍한 여성혐오 테러의 물결 뒤에는 겉으로 보이는 더 깊은 목적이 없었다. 그냥 알고리즘이 보상한 게 그런 콘텐츠였고, 그 참여가 같은 콘텐츠를 더 만들었다.

비슷한 일이 최근 틱톡에서도 있었는데, 이번에는 이민자들이 무자비하게 구타당하는 영상이 반응을 불러서 플랫폼 전반에 역겨운 콘텐츠가 급증했다.

가끔 이 효과는 비교적 benign하다(Shrimp Jesus 참고). 다른 때에는 반응을 유도하도록 만든 기계가 불가피하게 끔찍하고, 트라우마를 주고, 비인도적이고, 용납할 수 없는 것에서 “대박”을 친다.

AI는 그 자체로만 해롭지 않다. 기존 해악의 증폭기다. 그 뒤에 의도가 있든 없든 상관없다. 영향은 같다.

---

이 모든 게 우리 중 많은 이가 AI에 그렇게 비관적인 이유라고 생각한다. 우리에게, 우리가 아끼는 것들에게 위협이 되는 수많은 방식이 분명히 보인다.

많은 이에게 AI는 우리와 주변 사람에게서 중요한 것—안전, 안정, 창의성—을 빼앗아 가고, 그 대신 우리 위에 있는 사람들에게 주로 가는 생산성과 이익으로 대체한다.

많은 이가 AI에 반대하는 이유는, AI가 가장 가진 것이 적은 사람에게서 가져다가 이미 큰 특권을 가진 사람에게 더 주는 시스템으로 기능하는 게 보이기 때문이라고 생각한다.

> 많은 이가 AI에 반대하는 이유는, AI가 가장 가진 것이 적은 사람에게서 가져다가 이미 큰 특권을 가진 사람에게 더 주는 시스템으로 기능하는 게 보이기 때문이라고 생각한다.

그래서 AI가 모든 걸 고치고 노동자를 자유롭게 할 거라는 약속이 그렇게 중요하다. 전체 작전의 핵심이다. 가장 잃을 게 많은 사람들의 동의를 얻으려면 필요하다.

그다음은 그걸 이야기해 보자.

---

지금에 너무 집중해서 핵심을 완전히 놓치고 있다고 말할 수도 있다. 낙관론은 지금 일어나는 게 아니라 미래에 관한 거라고.

AI가 _현재_ 실제로 무엇인지는 잊어라. 모델은 나아지고, 데이터센터는 더 에너지 효율적이고, 토큰은 더 싸지고, 실수는 줄고, 해악은 완화되어 결국 세상을 더 나은 곳으로 바꿀, 우리 문제를 어떤 식으로든 풀어 줄 선의의 기술이 나올 거다. AGI(실제 인간 수준 인공지능; 2022년 전에 AI가 의미하던 것)이 되거나 그로 이어질 수도 있다.

이런 예측에는 몇 가지 이유로 반대한다.

* 기술과 그 비용이 계속 나아질 거라고 믿지만, 그게 이 해악 대부분을 어떻게 완화할지 보기 어렵다. 많은 해악은 속도·효율·저렴함이 커질수록 _더 심해질_ 가능성이 크다.
* 예측 전반에 회의적이지만,[^7] 특히 관찰된 현실과 거의 닮지 않은 예측에는 더 그렇다. 명확한 증거 연결이 없으면 예측은 순전히 과대광고와 추측에 기반하는데, AI 주변에는 둘 다 넘친다. 기술이 나아질 거라고 믿는 건 타당하다. 갑자기 새로운 무언가로 바뀌거나, 지금은 불가능한 능력을 갖추거나, 여기와 저기 사이에 명확한 경로나 연결 없이 우리를 먼 곳으로 데려갈 거라고 생각하는 건 훨씬 덜 타당해 보인다.
* AI에 대한 대부분의 유토피아적 상상은 AI가 지각이 있다는 생각에 중심을 두는데, AI는 사실적으로, 분명히 _그렇지 않다_. 언어와 통계가 인지를 쉽게 흉내 낼 수 있고, 우리 인간 뇌는 인간 행동을 애매하게 모방하는 건 뭐든 과하게 의인화하려 한다. 생각하고 추론하는 것과 소통을 통계적으로 흉내 내는 것은 매우 다르다.[^8]
* OpenAI와 Meta의 저명한 연구자들을 포함한 많은 LLM 전문가들이, 여러 이유(AI에 이미 오염되지 않은 학습 데이터의 고갈이 최소한 하나)로 모델이 이미 현실적인 한계에 가깝고 그 고원을 넘어 실질적으로 더 스케일하기 어렵다고 말한다. LLM이 AGI에 도달하는 데는 막다른 길이라고 말하는 이도 여러 명이고, LLM이 지어 내는 걸 막는 건 말 그대로 불가능하다고 대체로 동의한다. (실제로 OpenAI 사람들이 공개적으로 LLM이 “환각”을 멈추지 않을 거라고 인정했다. LLM이 작동하는 방식의 핵심이라 고칠 수 없는 버그다.) 그래서 LLM이 지능이 있거나 결국 있을 거라고 믿는다면, 모든 각도에서 당신보다 훨씬 더 아는 많은 사람과 맞서는 셈이다.
* 기술적 한계를 다 무시하거나 우회하더라도, 새로운 진보는 그렇게 일어난 적이 없다. (형평하고 노동자를 해방하는 방식으로는.) 기술은 노동자를 해방하지 않는다. 같은 시간에 더 많이 하게 하고, 같은 임금이나 더 낮은 임금으로 하게 한다. 두 배로 생산적이 되어도 임금이 두 배나 휴가가 두 배가 아니다. 그냥 업무량이 두 배다. 같은 일 하던 다른 사람이 레이오프당해서 그 일까지 하게 되는 식으로. 이런 기술은 하층 다수에게 불안정을 나눠 주고 상층에 이익을 모은다. AI보다 이걸 더 효율적으로 하는 메커니즘은 없었을 수 있다. 이번엔 다를 거라고 믿을 이유가 전혀 없다. _특히_ 다음 이유 때문에:
* AI 모델은 소수의 거대 기업 손에 집중되어 있고, 그 기업들은 자신들 노동자를 가능한 한 많이 없애는 데 전혀 꺼리지 않는다. AI는 기업 이익을 최우선으로 섬기고 있고 이미 그렇게 하고 있다. 웹 검색 같은 핵심 인프라를 계속 대체하면서 기업들이 원하는 대로 조작할 수 있기 _때문에_ 더 그렇다.[^9]
* 그렇지 않더라도, 순진하게 기술을 믿는다고 해도: 그 상상된 미래가 올 때까지 AI의 모든 해악과 위험을 _여전히_ 감수하는 셈이다. 그러면 원래 논점으로 돌아온다.

AI의 긍정적 사용 사례를 들 수도 있다. 접근성(accessibility)이 자주 나온다. (실제로 _그렇게_ 자주 나와서, 온라인 AI 옹호자들은 “ableist”라는 말만 꺼내면 논의를 닫을 수 있다는 걸 깨달았다.)

AI에 좋은 사용 사례는 있다. 대부분의 합리적인 사람이 그걸 반박하진 않을 것이다. 말했듯 나도 가끔 쓴다. 내 신체적 한계를 보완하려고. 하지만 그런 사례를 들면 AI의 다른 해악을 _장애인 등 AI로 이득 보는 사람을 이용해_ 정당화하는 악의적 시도가 되는 경우가 많다. 피해를 인정하고 AI의 배치를 재고해 _모두_ 를 위해 선을 최대화하고 해악을 최소화하는 게 아니다. 일부가 이득을 볼 수 있다고 해서 AI의 모든 사용과 모든 영향을 다 받아들일 필요는 없다.

마지막으로, _AI가 나쁜 데 쓰이긴 하지만 나는 그렇게 쓰지 않는다. 좋은 부분만 쓰고 내게 오는 이득만 즐기면 뭐가 문제냐_ 하고 생각할 수 있는 사람에게 한마디 하겠다.

그게 바로 특권이다. 당신이 말하는 게 문자 그대로 특권의 설명이다.

---

이 글을 시작한 것처럼 개인적인 예로 끝내겠다. 내 가족 이야기로.

나는 갓난 딸이 있다.

이 글을 쓰기 시작한 건 그녀가 태어나기 전이고, 대부분 그녀 때문에 몇 주 늦게 마무리하고 있다. (대부분 그녀가 옆에서 잠잘 때 낮잠 스케줄에 맞춰 타이핑했다.)

그리고 이 생각이 떠나지 않는다. 그녀에게 AI의 악의적 오용이 언젠가 향할 수 있는 세상에 그녀를 맞이하고 있다는.

이미 말한 것들을 넘어서, 기술 전반이 스토킹이나 학대를 이전보다 쉽게 만들었다. 하지만 AI는 더 나아간다. 인터넷만 있는 비열한 변태가 이 작은 아이의 딥페이크—포르노까지—를 동의 없이, 버튼 한 번에 만들 수 있는 세상을 알고 산다.

이게 끔찍하고 비정상적인 생각처럼 들리면, 그렇다. _완전히 그렇다._ 하지만 이걸 내가 혼자 떠올린 게 아니다. 이미 셀 수 없이 많은 여성, 그중 많은 이가 학령기 소녀들에게 벌어지고 있다.

AI 낙관주의자가 되려면 이걸 돌아서야 한다. 무시해야 한다. 전부 계획의 일부로, 조금 더 빨리 소프트웨어 쓰는 것과 바꿔서 (바라건대 다른 누군가가) 무심히 치를 대가로 여겨야 한다.

낙관하려면 내 아이들이 그런 경험을 하지 않을 거라고, 여기서 말한 다른 어떤 것도 하지 않을 거라고 믿어야 한다.

더 나은 학교, 더 나은 동네, 더 나은 친구, 더 나은 지원망에 있을 거라고 믿어야 한다.

잘못된 남자나 여자의 관심을 받지 않을 거라고.

잘못된 곳에 살지 않을 거라고. 시스템의 잘못된 쪽에 있지 않을 거라고.

가속하는 불평등의 잘못된 쪽에 있지 않을 거라고.

AI 낙관론은 당신과 당신이 아끼는 사람들을 AI로부터 안전하다고, 자율주행차의 _승객_ 이지 그게 치어버릴 _보행자_ 가 아니라고 보는 것을 요구한다.

> AI 낙관론은 당신과 당신이 아끼는 사람들을 AI로부터 안전하다고, 자율주행차의 승객이지 그게 치어버릴 보행자가 아니라고 보는 것을 요구한다.

계급 특권이 꽤 크지 않고서는 그렇게 자신을 볼 수 있을지 모르겠다.

나머지 우리는?

그 대가를 치를 사람들 가운데 당신이 있을 수 있다는 걸 알면, 그 편의가 그 대가만큼 가치할지—더 나아가 흥분할 만한지—보기 어렵다.

---

[^1] “AI”라고 말할 때 내가 실제로 의미하는 건 ChatGPT, Claude, Gemini, Copilot 같은 대규모 언어 모델(LLM)이다. 전쟁은 끝났고 용어가 널리 호환된다고 보지만, AI가 원래는 다른 뜻이었고 LLM은 실제로 지능이 아니어서, pedantic한 나는 이 구분을 항상 하고 싶어진다.

[^2] AI가 실제로는 느리게 하면서 _더 생산적이라고 느끼게_ 만든다는 증거가 있다. 내 경험과도 맞다. AI가 만든 걸 고치려고 원래 다 직접 했을 때보다 훨씬 더 많은 시간을 쓰는 경우가 많다.

[^3] AI가 노동자를 대체하는 능력은 과장되어 있다. 일부 작업은 처리할 수 있지만, 노동자는 작업 목록의 합 이상이고 그렇게 존재한다. 레이오프 후 재고용을 강제당한 일부 회사가 이미 깨달았다. 불행히도 대부분의 경영진은 시도하는 데 막히지 않는 것 같다.

[^4] 주니어를 AI로 대체하려는 건 이중으로 위험한 도박이다. 지금 주니어를 대체할 수 있을 뿐 아니라, 그들이 결국 성장할 시니어까지 대체할 수 있고, 기존 시니어가 떠나 자리가 비었을 때 그 자리까지 채울 수 있다고 믿어야 한다. 인재 파이프라인 전체를 한꺼번에 닫는 셈이다. 나라면 그 도박에 책임지고 싶지 않다.

[^5] AI의 환경 영향과 에너지 비용을 정확히 잡기 어렵고 추정치 범위가 넓다는 걸 안다. 궁극적으로 영향이 상대적으로 작더라도—AI 기업들의 불투명함을 생각하면 그렇게 생각하지는 않지만, 그렇다 해도—우리 행성이 이미 감당할 수 있는 한계를 넘어가는 지금, AI 없을 때보다 확실히 걱정할 만한 규모다. 게다가 어제만 해도 미국 두 지역의 두 데이터센터가 인근 도시 주민에게 희귀암·유산을 유발할 가능성이 있다는 뉴스 두 건을 봤다. 넓은 환경 영향과 관계없이 다른 해악도 고려해야 할 것 같다.

[^6] 할머니가 tech-savvy일 수 있고 젊은이도 아닐 수 있지만, 누구나 사기에 당할 수 있다는 고정관념을 부추기려는 건 아니다.

[^7] 누구나 뭐든 예측할 수 있다. 이 글이 퓰리처상을 받을 거라고 나는 예측한다. 봤지?

[^8] AI가 지각이 있다고 주장하는 사람들이 대개 기술 쪽에 있고, 인지·지능 등을 실제 학문으로 연구하는 사람들이 아니라는 걸 알 수 있다. 신경과학 등 여러 분야에 그런 전문가가 많고, 대부분은 아니다, 그게 사고가 아니며, 우리는 뇌가 어떻게 작동하는지 아직 완전히 이해하지도 못한다고 말한다. 하지만 기술 쪽 사람들이 진짜 전문가에게 묻는 걸 거의 생각하지 않는다는 것도 알 수 있다. 대부분 코드 전문가가 다른 모든 것의 전문가이기도 하다고 가정하고, 뇌를 이해했고 하나 만들었다고 자신 있게 말한다.

[^9] AI 출력이 한 사람이나 기업의 의제에 맞게 조작될 수 있다고 믿지 않는다면, 최근 Grok이 뭘 하는지 한번 보라고 권한다.
